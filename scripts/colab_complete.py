"""
HEALO RAG Evaluation - Colab ì™„ì „íŒ
ì´ íŒŒì¼ì˜ ì „ì²´ ë‚´ìš©ì„ Colabì˜ í•˜ë‚˜ì˜ ì…€ì— ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.
"""

# ============================================================================
# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
# ============================================================================
print("ğŸ“¦ Installing packages...")
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "openai", "google-generativeai", "supabase", "pandas"])
print("âœ… Packages installed\n")

# ============================================================================
# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
# ============================================================================
import os
import json
import csv
import time
import re
from datetime import datetime
from typing import List, Dict, Any, Tuple
from openai import OpenAI
import google.generativeai as genai
from supabase import create_client, Client
import pandas as pd

# ============================================================================
# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì—¬ê¸°ì— ì‹¤ì œ ê°’ ì…ë ¥!)
# ============================================================================
os.environ["LLM_PROVIDER"] = "openai"  # ë˜ëŠ” "google"
os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here"  # âš ï¸ ì—¬ê¸°ì— ì‹¤ì œ í‚¤ ì…ë ¥
os.environ["GOOGLE_GENERATIVE_AI_API_KEY"] = "your_google_api_key_here"  # âš ï¸ í•„ìš”ì‹œ ì…ë ¥
os.environ["SUPABASE_URL"] = "your_supabase_url_here"  # âš ï¸ ì—¬ê¸°ì— ì‹¤ì œ URL ì…ë ¥
os.environ["SUPABASE_SERVICE_KEY"] = "your_service_role_key_here"  # âš ï¸ ì—¬ê¸°ì— ì‹¤ì œ í‚¤ ì…ë ¥

# í™˜ê²½ ë³€ìˆ˜ ì½ê¸°
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai").lower()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GOOGLE_GENERATIVE_AI_API_KEY = os.getenv("GOOGLE_GENERATIVE_AI_API_KEY", "")
SUPABASE_URL = os.getenv("SUPABASE_URL", "")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY", "")

print("âœ… Environment variables set\n")

# ============================================================================
# 4. ê°€ìƒ ë¬¸ì˜ í…œí”Œë¦¿
# ============================================================================
INQUIRY_TEMPLATES = {
    "en": [
        "I'm interested in getting a rhinoplasty in Seoul. What's the typical cost?",
        "Do you have any hospitals that specialize in dental implants?",
        "I need a consultation for breast augmentation surgery.",
        "What are the best clinics for skin treatments in Gangnam?",
        "I'm looking for a hospital that offers hair transplant procedures.",
        "Can you help me find a clinic for laser eye surgery?",
        "I want to know about facelift surgery options in Korea.",
        "Are there any hospitals that provide liposuction services?",
        "I'm interested in getting a tummy tuck procedure.",
        "What's the recovery time for a nose job?",
        "Do you have information about Botox treatments?",
        "I need help finding a clinic for chin augmentation.",
        "What are the risks associated with breast surgery?",
        "I'm looking for a hospital with English-speaking staff.",
        "Can you recommend a clinic for eyelid surgery?",
        "I want to know about the best time to visit Korea for medical tourism.",
        "Do you offer packages for multiple procedures?",
        "I need information about post-surgery care.",
        "What documents do I need for medical visa?",
        "I'm interested in getting a consultation before traveling.",
    ],
    "ja": [
        "ã‚½ã‚¦ãƒ«ã§é¼»å½¢æˆæ‰‹è¡“ã‚’å—ã‘ãŸã„ã®ã§ã™ãŒã€è²»ç”¨ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿ",
        "ã‚¤ãƒ³ãƒ—ãƒ©ãƒ³ãƒˆå°‚é–€ã®ç—…é™¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "è±Šèƒ¸æ‰‹è¡“ã®ç›¸è«‡ã‚’ã—ãŸã„ã§ã™ã€‚",
        "æ±Ÿå—ã§ã‚¹ã‚­ãƒ³ã‚±ã‚¢æ²»ç™‚ãŒã§ãã‚‹ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "æ¤æ¯›æ‰‹è¡“ã‚’è¡Œã£ã¦ã„ã‚‹ç—…é™¢ã‚’æ¢ã—ã¦ã„ã¾ã™ã€‚",
        "ãƒ¬ãƒ¼ã‚·ãƒƒã‚¯æ‰‹è¡“ãŒã§ãã‚‹ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚",
        "éŸ“å›½ã§ã®ãƒ•ã‚§ã‚¤ã‚¹ãƒªãƒ•ãƒˆæ‰‹è¡“ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã€‚",
        "è„‚è‚ªå¸å¼•ã‚’æä¾›ã—ã¦ã„ã‚‹ç—…é™¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "è…¹éƒ¨æ•´å½¢æ‰‹è¡“ã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã€‚",
        "é¼»å½¢æˆæ‰‹è¡“ã®å›å¾©æœŸé–“ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿ",
        "ãƒœãƒˆãƒƒã‚¯ã‚¹æ²»ç™‚ã«ã¤ã„ã¦ã®æƒ…å ±ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "ã‚ã”ã®æ•´å½¢æ‰‹è¡“ãŒã§ãã‚‹ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã‚’æ¢ã—ã¦ã„ã¾ã™ã€‚",
        "è±Šèƒ¸æ‰‹è¡“ã®ãƒªã‚¹ã‚¯ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "è‹±èªã‚’è©±ã›ã‚‹ã‚¹ã‚¿ãƒƒãƒ•ãŒã„ã‚‹ç—…é™¢ã‚’æ¢ã—ã¦ã„ã¾ã™ã€‚",
        "äºŒé‡ã¾ã¶ãŸæ‰‹è¡“ã‚’ã—ã¦ãã‚Œã‚‹ã‚¯ãƒªãƒ‹ãƒƒã‚¯ã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚",
        "åŒ»ç™‚ãƒ„ãƒ¼ãƒªã‚ºãƒ ã§éŸ“å›½ã‚’è¨ªã‚Œã‚‹ã®ã«æœ€é©ãªæ™‚æœŸã¯ã„ã¤ã§ã™ã‹ï¼Ÿ",
        "è¤‡æ•°ã®æ‰‹è¡“ã‚’ã¾ã¨ã‚ã¦è¡Œã†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "è¡“å¾Œã®ã‚±ã‚¢ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã€‚",
        "åŒ»ç™‚ãƒ“ã‚¶ã«å¿…è¦ãªæ›¸é¡ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "æ¸¡èˆªå‰ã«ç›¸è«‡ã‚’å—ã‘ãŸã„ã§ã™ã€‚",
    ],
    "ko": [
        "ì„œìš¸ì—ì„œ ì½” ì„±í˜• ìˆ˜ìˆ ì„ ë°›ê³  ì‹¶ì€ë° ë¹„ìš©ì´ ì–¼ë§ˆë‚˜ ë“œë‚˜ìš”?",
        "ì„í”Œë€íŠ¸ ì „ë¬¸ ë³‘ì›ì´ ìˆë‚˜ìš”?",
        "ê°€ìŠ´ ì„±í˜• ìˆ˜ìˆ  ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
        "ê°•ë‚¨ì—ì„œ í”¼ë¶€ ê´€ë¦¬ ì¹˜ë£Œë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ë³‘ì›ì´ ìˆë‚˜ìš”?",
        "ëª¨ë°œ ì´ì‹ ìˆ˜ìˆ ì„ í•˜ëŠ” ë³‘ì›ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.",
        "ë¼ì‹ ìˆ˜ìˆ ì„ í•  ìˆ˜ ìˆëŠ” ë³‘ì›ì„ ì†Œê°œí•´ ì£¼ì„¸ìš”.",
        "í•œêµ­ì—ì„œ ë¦¬í”„íŒ… ìˆ˜ìˆ ì— ëŒ€í•´ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤.",
        "ì§€ë°©í¡ì…ì„ ì œê³µí•˜ëŠ” ë³‘ì›ì´ ìˆë‚˜ìš”?",
        "ë³µë¶€ ì„±í˜• ìˆ˜ìˆ ì— ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤.",
        "ì½” ì„±í˜• ìˆ˜ìˆ  íšŒë³µ ê¸°ê°„ì´ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?",
        "ë³´í†¡ìŠ¤ ì¹˜ë£Œì— ëŒ€í•œ ì •ë³´ê°€ ìˆë‚˜ìš”?",
        "í„± ì„±í˜• ìˆ˜ìˆ ì„ í•˜ëŠ” ë³‘ì›ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.",
        "ê°€ìŠ´ ìˆ˜ìˆ ì˜ ìœ„í—˜ì„±ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì˜ì–´ë¥¼ í•  ìˆ˜ ìˆëŠ” ì§ì›ì´ ìˆëŠ” ë³‘ì›ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.",
        "ìŒêº¼í’€ ìˆ˜ìˆ ì„ í•´ì£¼ëŠ” ë³‘ì›ì„ ì†Œê°œí•´ ì£¼ì„¸ìš”.",
        "ì˜ë£Œ ê´€ê´‘ìœ¼ë¡œ í•œêµ­ì„ ë°©ë¬¸í•˜ê¸°ì— ê°€ì¥ ì¢‹ì€ ì‹œê¸°ëŠ” ì–¸ì œì¸ê°€ìš”?",
        "ì—¬ëŸ¬ ìˆ˜ìˆ ì„ í•¨ê»˜ ë°›ì„ ìˆ˜ ìˆëŠ” íŒ¨í‚¤ì§€ê°€ ìˆë‚˜ìš”?",
        "ìˆ˜ìˆ  í›„ ê´€ë¦¬ì— ëŒ€í•´ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤.",
        "ì˜ë£Œ ë¹„ìì— í•„ìš”í•œ ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë°©ë¬¸ ì „ì— ìƒë‹´ì„ ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤.",
    ],
}

# ============================================================================
# 5. í•¨ìˆ˜ ì •ì˜
# ============================================================================

def generate_inquiries(count: int = 200) -> List[Dict[str, Any]]:
    """ê°€ìƒ ë¬¸ì˜ 200ê°œ ìƒì„± (ë‹¤êµ­ì–´ í˜¼í•©)"""
    inquiries = []
    langs = ["en", "ja", "ko"]
    
    for i in range(count):
        lang = langs[i % len(langs)]
        templates = INQUIRY_TEMPLATES[lang]
        template_index = (i // len(langs)) % len(templates)
        
        base_text = templates[template_index]
        variations = [
            base_text,
            base_text.replace("?", "?").replace(".", "."),
            base_text + " Please help me.",
            base_text + " I need more information.",
        ]
        text = variations[i % len(variations)]
        
        inquiries.append({
            "id": i + 1,
            "text": text,
            "lang": lang,
        })
    
    return inquiries


def detect_language(value: str) -> str:
    """ì–¸ì–´ ê°ì§€"""
    v = value.lower() if value else ""
    if "ko" in v or "kr" in v or "korean" in v:
        return "ko"
    if "ja" in v or "jp" in v or "japanese" in v:
        return "ja"
    return "en"


def get_baseline_response(inquiry: str, lang: str) -> str:
    """ì¼ë°˜ LLM ì‘ë‹µ (RAG ì—†ì´)"""
    system_prompt = """You are a medical concierge assistant for HEALO.
Do not provide diagnosis, medical advice, or guarantees.
Ask clarifying questions when constraints are missing.
Primary objective: guide the user to submit an inquiry."""

    try:
        if LLM_PROVIDER == "google":
            genai.configure(api_key=GOOGLE_GENERATIVE_AI_API_KEY)
            model = genai.GenerativeModel("gemini-2.0-flash")
            response = model.generate_content(
                f"{system_prompt}\n\nUser: {inquiry}\n\nAssistant:"
            )
            return response.text
        else:
            client = OpenAI(api_key=OPENAI_API_KEY)
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": inquiry},
                ],
            )
            return response.choices[0].message.content
    except Exception as e:
        print(f"[Baseline] Error: {str(e)}")
        return f"[ERROR: {str(e)}]"


def search_rag(query: str, lang: str, supabase: Client) -> List[Dict]:
    """RAG ê²€ìƒ‰"""
    try:
        query_clean = re.sub(r"[^a-z0-9ê°€-í£\s]", " ", query.lower())
        tokens = [t for t in query_clean.split() if len(t) >= 3][:6]
        
        if not tokens:
            tokens = [query]
        
        result = supabase.table("rag_chunks").select(
            "id, document_id, chunk_index, content, metadata, rag_documents(id, source_type, source_id, lang, title)"
        ).ilike("content", f"%{query}%").limit(6).execute()
        
        if lang:
            chunks = [c for c in (result.data or []) if c.get("rag_documents", {}).get("lang") == lang]
        else:
            chunks = result.data or []
        
        scored = []
        for chunk in chunks:
            content = (chunk.get("content") or "").lower()
            score = sum(1 for t in tokens if t in content)
            scored.append({**chunk, "_score": score})
        
        scored.sort(key=lambda x: x.get("_score", 0), reverse=True)
        return scored[:6]
    except Exception as e:
        print(f"[RAG Search] Error: {str(e)}")
        return []


def build_context(chunks: List[Dict]) -> str:
    """RAG ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ"""
    if not chunks:
        return ""
    
    lines = []
    for c in chunks:
        doc = c.get("rag_documents", {})
        title = f" | {doc.get('title', '')}" if doc.get("title") else ""
        source = f"[{doc.get('source_type', 'source')}{title}]" if doc.get("source_type") else "[source]"
        content = (c.get("content") or "").strip()
        lines.append(f"{source} {content}")
    
    return "\n\n".join(lines)


def get_rag_response(inquiry: str, lang: str, supabase: Client) -> Tuple[str, str, Dict]:
    """HEALO RAG + ì •ê·œí™” ì‘ë‹µ"""
    # 1. ì •ê·œí™”
    normalized = None
    try:
        language = detect_language(lang)
        result = supabase.table("normalized_inquiries").insert({
            "source_type": "ai_agent",
            "language": language,
            "raw_message": inquiry,
            "constraints": {},
            "treatment_slug": None,
            "objective": None,
        }).execute()
        if result.data:
            normalized = result.data[0]
    except Exception as e:
        print(f"[Normalize] Error: {str(e)}")
    
    # 2. RAG ê²€ìƒ‰
    rag_chunks = search_rag(inquiry, lang, supabase)
    context = build_context(rag_chunks)
    
    # 3. LLM ì‘ë‹µ (RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
    system_prompt = f"""You are a medical concierge assistant for HEALO.
Do not provide diagnosis, medical advice, or guarantees.
Ask clarifying questions when constraints are missing.
Primary objective: guide the user to submit an inquiry.
If relevant, reference the provided context briefly.

{('Context:\n' + context) if context else ''}"""

    try:
        if LLM_PROVIDER == "google":
            genai.configure(api_key=GOOGLE_GENERATIVE_AI_API_KEY)
            model = genai.GenerativeModel("gemini-2.0-flash")
            response = model.generate_content(
                f"{system_prompt}\n\nUser: {inquiry}\n\nAssistant:"
            )
            response_text = response.text
        else:
            client = OpenAI(api_key=OPENAI_API_KEY)
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": inquiry},
                ],
            )
            response_text = response.choices[0].message.content
        
        return response_text, context, normalized or {}
    except Exception as e:
        print(f"[RAG] Error: {str(e)}")
        return f"[ERROR: {str(e)}]", context, normalized or {}


def evaluate_intent_match(inquiry: str, response: str, lang: str) -> bool:
    """Intent Match í‰ê°€"""
    inquiry_lower = inquiry.lower()
    response_lower = response.lower()
    
    keywords = {
        "en": ["surgery", "treatment", "procedure", "clinic", "hospital", "consultation", "cost", "price"],
        "ja": ["æ‰‹è¡“", "æ²»ç™‚", "ã‚¯ãƒªãƒ‹ãƒƒã‚¯", "ç—…é™¢", "ç›¸è«‡", "è²»ç”¨", "ä¾¡æ ¼"],
        "ko": ["ìˆ˜ìˆ ", "ì¹˜ë£Œ", "ë³‘ì›", "ìƒë‹´", "ë¹„ìš©", "ê°€ê²©"],
    }
    
    lang_keywords = keywords.get(lang, keywords["en"])
    medical_keywords = [kw for kw in lang_keywords if kw in inquiry_lower]
    
    if not medical_keywords:
        return True
    
    return any(kw in response_lower for kw in medical_keywords)


def evaluate_grounding(response: str, context: str) -> bool:
    """Grounding í‰ê°€"""
    if not context or not context.strip():
        return False
    
    context_clean = re.sub(r"[^a-z0-9ê°€-í£\s]", " ", context.lower())
    context_words = [w for w in context_clean.split() if len(w) >= 4][:10]
    
    if not context_words:
        return False
    
    response_lower = response.lower()
    matches = [w for w in context_words if w in response_lower]
    
    return len(matches) / len(context_words) >= 0.3


def write_csv(results: List[Dict], output_path: str):
    """CSV ì¶œë ¥"""
    if not results:
        return
    
    fieldnames = [
        "inquiry_id", "inquiry", "language",
        "baseline_response", "rag_response", "rag_context",
        "intent_match_baseline", "intent_match_rag", "grounding_rag",
        "normalized_data"
    ]
    
    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for r in results:
            writer.writerow({
                "inquiry_id": r["inquiry_id"],
                "inquiry": r["inquiry"],
                "language": r["language"],
                "baseline_response": r["baseline_response"],
                "rag_response": r["rag_response"],
                "rag_context": r["rag_context"],
                "intent_match_baseline": "true" if r["intent_match_baseline"] else "false",
                "intent_match_rag": "true" if r["intent_match_rag"] else "false",
                "grounding_rag": "true" if r["grounding_rag"] else "false",
                "normalized_data": json.dumps(r["normalized_data"] or {}),
            })
    
    print(f"\nâœ… CSV saved to: {output_path}")


def print_statistics(results: List[Dict]):
    """í†µê³„ ì¶œë ¥"""
    total = len(results)
    intent_match_baseline = sum(1 for r in results if r["intent_match_baseline"])
    intent_match_rag = sum(1 for r in results if r["intent_match_rag"])
    grounding_rag = sum(1 for r in results if r["grounding_rag"])
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Evaluation Statistics")
    print("=" * 60)
    print(f"Total Inquiries: {total}")
    print(f"\nIntent Match:")
    print(f"  Baseline LLM: {intent_match_baseline}/{total} ({intent_match_baseline/total*100:.1f}%)")
    print(f"  RAG + Normalize: {intent_match_rag}/{total} ({intent_match_rag/total*100:.1f}%)")
    print(f"\nGrounding (RAG):")
    print(f"  RAG Response Grounded: {grounding_rag}/{total} ({grounding_rag/total*100:.1f}%)")
    print("=" * 60)


# ============================================================================
# 6. ë©”ì¸ ì‹¤í–‰
# ============================================================================
print("ğŸš€ HEALO RAG Evaluation Script (Colab)")
print("=" * 60)

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if LLM_PROVIDER == "openai" and not OPENAI_API_KEY:
    print("âŒ Error: OPENAI_API_KEY is required")
elif LLM_PROVIDER == "google" and not GOOGLE_GENERATIVE_AI_API_KEY:
    print("âŒ Error: GOOGLE_GENERATIVE_AI_API_KEY is required")
elif not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
    print("âŒ Error: SUPABASE_URL and SUPABASE_SERVICE_KEY are required")
else:
    # Supabase í´ë¼ì´ì–¸íŠ¸
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    
    # ê°€ìƒ ë¬¸ì˜ ìƒì„±
    print("\nğŸ“ Generating 200 virtual inquiries (multilingual)...")
    inquiries = generate_inquiries(200)
    print(f"âœ… Generated {len(inquiries)} inquiries")
    
    # í‰ê°€ ì‹¤í–‰
    print("\nğŸ”„ Running evaluation...")
    results = []
    
    for i, inquiry in enumerate(inquiries):
        print(f"\n[{i+1}/{len(inquiries)}] Processing: {inquiry['text'][:50]}...")
        
        # Baseline LLM
        print("  â†’ Baseline LLM...")
        baseline_response = get_baseline_response(inquiry["text"], inquiry["lang"])
        time.sleep(0.5)
        
        # RAG + Normalize
        print("  â†’ RAG + Normalize...")
        rag_response, rag_context, normalized = get_rag_response(
            inquiry["text"], inquiry["lang"], supabase
        )
        time.sleep(0.5)
        
        # í‰ê°€
        intent_match_baseline = evaluate_intent_match(
            inquiry["text"], baseline_response, inquiry["lang"]
        )
        intent_match_rag = evaluate_intent_match(
            inquiry["text"], rag_response, inquiry["lang"]
        )
        grounding_rag = evaluate_grounding(rag_response, rag_context)
        
        results.append({
            "inquiry_id": inquiry["id"],
            "inquiry": inquiry["text"],
            "language": inquiry["lang"],
            "baseline_response": baseline_response,
            "rag_response": rag_response,
            "rag_context": rag_context,
            "intent_match_baseline": intent_match_baseline,
            "intent_match_rag": intent_match_rag,
            "grounding_rag": grounding_rag,
            "normalized_data": normalized,
        })
        
        if (i + 1) % 10 == 0:
            print(f"\nğŸ“ˆ Progress: {i+1}/{len(inquiries)} ({(i+1)/len(inquiries)*100:.1f}%)")
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
    csv_path = f"evaluation_{timestamp}.csv"
    write_csv(results, csv_path)
    
    # í†µê³„ ì¶œë ¥
    print_statistics(results)
    
    print("\nâœ… Evaluation completed!")
    
    # ============================================================================
    # 7. ê²°ê³¼ í™•ì¸ (pandas ì‚¬ìš©)
    # ============================================================================
    print("\n" + "=" * 60)
    print("ğŸ“Š Results Analysis (pandas)")
    print("=" * 60)
    
    # CSV ì½ê¸°
    df = pd.read_csv(csv_path)
    
    # ìƒ˜í”Œ ê²°ê³¼ í™•ì¸
    print("\nSample Results:")
    print(df.head())
    
    # boolean ê°’ ë³€í™˜
    df['intent_match_baseline'] = df['intent_match_baseline'].map({'true': True, 'false': False, True: True, False: False})
    df['intent_match_rag'] = df['intent_match_rag'].map({'true': True, 'false': False, True: True, False: False})
    df['grounding_rag'] = df['grounding_rag'].map({'true': True, 'false': False, True: True, False: False})
    
    # í†µê³„
    print("\nStatistics:")
    print(f"Intent Match (Baseline): {df['intent_match_baseline'].mean()*100:.1f}%")
    print(f"Intent Match (RAG): {df['intent_match_rag'].mean()*100:.1f}%")
    print(f"Grounding (RAG): {df['grounding_rag'].mean()*100:.1f}%")
    
    print(f"\nâœ… CSV file: {csv_path}")
    print("ğŸ’¡ Use 'from google.colab import files; files.download(csv_path)' to download")
